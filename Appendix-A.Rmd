```{=html}
<style type="text/css">
details:hover { cursor: pointer }
</style>
```
---
author: "Benjamin Meyer, Kenai Watershed Forum"
date: "`r Sys.Date()`"
output: html_document:
  code_folding: hide
output: pdf_book
---

# (APPENDIX) Appendix {.unnumbered}

# Appendix: Data Review and Uplift

## Introduction

Prior to analysis and interpretation of water quality data, we will ensure that all data that meets QA/QC standards outlined in the current project [Quality Assurance Project Plan (QAPP)](https://paperpile.com/app/p/7703451b-460d-00b4-82a0-1086ea2554c3) is accessible in the appropriate repository.

Water quality data from this project is ultimately destined for the EPA Water Quality Exchange (EPA WQX), formerly EPA STORET. The process of transferring these data to the higher-level EPA repository is described as data "Uplift."

Section B10 of the 2020 QAPP describes data management details and responsible parties for each step of the data pipeline from observation to repository. The 2021 data review process is published here as an appendix as an example of the process applied annually to each year's data.

### 2021 Water Quality Data

Water quality data generated from the 2021 Kenai River Baseline Water Quality Monitoring (KRBWQM) program will be submitted to the Soldotna office of the Alaska Department of Environmental Conservation (ADEC) in April 2022 using the project-specific spreadsheet template provided by ADEC. The template is intended to prepare data for submission to the [Ambient Water Quality Data Management System](https://www.awqms.com/), the service that ADEC uses to manage water quality data.

```{r, echo = F}
xfun::embed_file('other/input/AQWMS/AWQMS_KWF_Baseline_2021.xlsx', text = "Download the AQWMS Water Quality Data Spreadsheet Template")
```

#### 2021 Water Quality Data AQWMS Formatting

The code scripts below assemble water quality data from the three analytical laboratories that partnered with Kenai Watershed Forum for this project in 2021:

-   SGS Laboratories (Anchorage, AK)

-   Soldotna Wastewater Treatment Plant (Soldotna, AK)

-   Taurianen Engineering and Testing (Soldotna, AK)

<br>

------------------------------------------------------------------------

```{r, include=FALSE, eval = FALSE}

# this code hides the print output and content of ALL code chunks
# when rendering PDF version, set this chunk option to eval = TRUE

knitr::opts_chunk$set(
  include = FALSE
)
```

##### Metals/Nutrients Lab Results (SGS Labs)

```{r, echo = F}
xfun::embed_file('other/input/2021_wqx_data/spring_2021_wqx_data/SGS/spring_2021_sgs_batch_info.csv', text = "Download Original Spring 2021 Metals/Nutrients Lab Results from SGS")

```

```{r, echo = F}
xfun::embed_file('other/input/2021_wqx_data/summer_2021_wqx_data/SGS/summer_2021_sgs_batch_info.csv', text = "Download Original Summer 2021 Metals/Nutrients Lab Results from SGS")

```

<details>

<summary>

*Show/Hide Code used to Prepare 2021 Metals/Nutrients Results*

</summary>

```{r, 2021 AQWMS formatting for SGS}

# clear environment
rm(list=ls())

# load packages
library(tidyverse)
library(readxl)
library(openxlsx)
library(data.table)
library(stringr)
library(magrittr)
library(janitor)
library(hms)
library(lubridate)
library(anytime)

xfun::pkg_load2(c("htmltools", "mime"))


# Assign 2021 Field Sample Dates 

# Spring 2021 sampling date
spring21_sample_date <- "5/11/2021"

# Summer 2021 Sampling Date
summer21_sample_date <-"7/27/2021"
```

```{r}
################################################################################################################
######################################### Read in  and Clean SGS/ALS Data ######################################
################################################################################################################


############################ Part A: SGS Data Read In #############################

## Reformat SGS data downloaded from their server client (SGS Engage, full EDD files) to match AQWMS template

# read in
spring_batch_sgs21 <- read.csv("other/input/2021_wqx_data/spring_2021_wqx_data/SGS/spring_2021_sgs_batch_info.csv")
summer_batch_sgs21 <- read.csv("other/input/2021_wqx_data/summer_2021_wqx_data/SGS/summer_2021_sgs_batch_info.csv")

# clean up and retain only useful columns
sgs21 <- bind_rows(spring_batch_sgs21,summer_batch_sgs21) %>%
  clean_names() %>%
  remove_empty() %>%
  
  # remove unneeded columns
  select(-project_id)%>%
  rename(sample = sample_id,
         lab_sample = lab_sample_id,
         detection_limit = dl) %>%
  transform(lab_sample = as.character(lab_sample),
            sample_rpd = as.character(sample_rpd)) %>%
  # add lab name
  mutate(lab_name = "SGS North America, Anchorage, Alaska",
         matrix = "Water") %>%
  
  # split a.) lab sample run & b.) collect time and date in prep for future join with ALS data
          ##### NOTE: SGS data has date and time, ALS has date only. 
  transform(collect_date_time = mdy_hm(collect_date),
            rec_date_time = mdy_hm(rec_date),
            run_date_time = mdy_hm(run_date_time),
            extracted_date_time = mdy_hm(extracted_date)) %>%
  mutate(collect_time = as_hms(collect_date_time),
         collect_date = date(collect_date_time),
         rec_date = date(rec_date_time),
         rec_time = as_hms(rec_date_time),
         run_time = as_hms(run_date_time),
         run_date = date(run_date_time),
         extracted_time = as_hms(extracted_date_time),
         extracted_date = date(extracted_date_time)) %>%
  select(-collect_date_time,-rec_date_time,-run_date_time) %>%
  rename(sample = sample)

rm(spring_batch_sgs21,summer_batch_sgs21)




###################### Part B: ALS Data Read In #############################

## SGS subcontracted analyses of Ca, Fe, and Mg to ALS laboratories (Kelso, WA). These results are not included in the spreadsheet download from SGS engage and were entered manually in to seperate spring and summer "ALS" named spreadsheets


#### read in spring 2021 results from ALS 
spring_als21 <- read.csv("other/input/2021_wqx_data/spring_2021_wqx_data/SGS/spring_2021_als_batch_info.csv") %>%
    clean_names() 
summer_als21 <- read.csv("other/input/2021_wqx_data/summer_2021_wqx_data/SGS/summer_2021_als_batch_info.csv") %>%
    clean_names()


# bind spring and summer
als21 <- bind_rows(spring_als21,summer_als21) %>%
  remove_empty() %>%
  
  # proceed left to right of existing ALS dataframe to make its naming structure match the sgs21 dataframe. Add, remove, modify column names as needed
  select(-client,
         -project,
         -service_request) %>%
  rename(lab_sample = lab_code) %>%
  rename(
         collect_date = date_collected,
         collect_time = time_collected,
         rec_date = date_received,
         rec_time = time_received,
         # sample_type ::: not sure where to match with sgs data yet or where to put in aqwms, but is important for qa/qc
         
         extracted_date = date_extracted,
         extracted_time = time_extracted,
         extraction_code = extraction_method,
         run_date = date_analyzed,
         run_time = time_analyzed,
         analytical_method = method,
         #units = units,
         analyte = component,
         resultflag = result_notes,
         amount_spiked = spike_concentration,
         percent_recovered = percent_recovery,
         allowable_limit = acceptance_limits,
         sample_rpd = rpd,
        # change report/detection limit terminology  See SGS document, "SGS DL, LOD, LOQ Interpretation"
         loq = reporting_limit) %>%
  
  mutate(lab_name = "ALS Environmental, Kelso, Washington"
         #,
         #run_time = ""
         ) %>%
  # prep column classes to bind with sgs dataframe
  transform(analytical_method = as.character(analytical_method),
            run_date = mdy(run_date),
            run_time = as_hms(as.POSIXct(run_time, format = "%H:%M")),
            #run_time = as_hms(run_time),
            collect_date = mdy(collect_date),
            rec_date = mdy(rec_date),
            rec_time = as_hms(as.POSIXct(rec_time, format = "%H:%M")),
            extracted_date = mdy(extracted_date),
            extracted_time = as_hms(as.POSIXct(extracted_time, format = "%H:%M")),
            result = as.double(result),
            collect_time = as_hms(as.POSIXct(collect_time, format = "%H:%M")))



# join SGS data with ALS data
dat <- bind_rows(sgs21,als21)  


rm(als21,sgs21,spring_als21,summer_als21)

# export table of sample types

## assign sample type acronyms just like with sgs21 samples. see excel file for full definitions

# --> make sure doesn't conflict with other sample_type designations in rest of document. use same acronyms


### export table of sample types, then manually translate their abbreviations
sample_types <- dat %>%
  select(sample_type,lab_name) %>%
  distinct()

# remove old version and write new one
unlink("other/input/AQWMS/sample_type_abbreviations.xlsx")
write.xlsx(sample_types, "other/input/AQWMS/sample_type_abbreviations.xlsx")

# manually created a translation of all the acronyms in an accompanying file. removed inconsistencies in sample type abbreviations into one consistent schema between SGS and ALS labs




############### Part C: Address spelling/format issues and inconsistent sample/site names ######################

# Upon visual inspection of site names, we can see that the location names in the AQWMS template differ slightly from the place names in the SGS report (spelling and name inconsistencies).



# 3/28/2022 - A note on addressing "Duplicate" designations. In QA/QC data review in March 2022 the following was clarified through trial and error: we must make a careful distinction between "Field Duplicates" and "Lab duplicates" when preparing this data. The sample names contain info about whether a result is from a "Field Duplicate," e.g., two field collections made at the same location/day/time. However the ALS lab also created "Lab Duplicates," which are not from the same sites as field duplicates, and designates these as "DUP1" in the "sample_type" column.

# See AQWMS Activity Type column assign distinctions

# Decision - we will designate the field duplicates simply as a Field Duplicate


# move info about duplicate sample and/or sample blank status into separate column, "sample_condition"
dat %<>%
  mutate(sample_condition = case_when(
    grepl("Method Blank",sample) ~ "Method Blank",
    grepl("Trip Blank",sample) ~ "Trip Blank",
    grepl("DUP",sample) ~ "Field Duplicate",
    grepl("Dup",sample) ~ "Field Duplicate")) %>%
  # remove "DUP" designation from sample column
  mutate(sample = str_replace(sample, "DUP|Dup", "")) 

  
# remove from "sample" names the text containing the suffixes Diss/Dis (Dissolved metals sample) since we only  want location info in this column. (Solution for this step was found at https://stackoverflow.com/questions/29271549/replace-all-occurrences-of-a-string-in-a-data-frame)
dat %<>%  
  mutate(sample = (str_replace(sample, "Diss|Dis|DUP|Dup",""))) %>%
  
  # remove "Diss" suffix and "EP" prefix from "analytical_method" column
  mutate(analytical_method = str_replace(analytical_method, "Diss", "")) %>%
  # note trailing space after "EP200.8 "
  mutate(analytical_method = str_replace(analytical_method,"EP200.8 ","200.8")) %>%

  # address the one stubborn site name still containing "Diss"
  mutate(sample = case_when(
    sample == "RM0-No Name Creek  Diss" ~ "RM0-No Name Creek",
    TRUE ~ sample)) 
  
  
# We need to remove white spaces, apostrophes, and dashes; because join functions such as "left_join" are often uncooperative with these types of string characters. We will need to use joins with site names in next steps.
  
dat %<>%
    # remove excess white spaces
  mutate(sample = str_trim(sample,"both")) %>%
  mutate(sample = str_squish(sample)) %>%
  
  # make remaining white spaces underscores
  mutate(sample = gsub("\\s+","_",sample)) %>%
  
  # remove apostrophes
  mutate(sample = gsub("\\'","",sample)) %>%
  
  # replace dashes with underscores
  mutate(sample = gsub("\\-","_",sample)) %>%
  
  # replace multiple underscores with single
  mutate(sample = gsub("\\__","_",sample)) %>%
  mutate(sample = gsub("\\___","_",sample)) %>%

  # again replace multiple underscores with single
  mutate(sample = gsub("\\__","_",sample)) 

# apply note regarding trip blanks (for BTEX organics)
# assigned in sequence as encountered on chain of custody
dat %<>%
  mutate(note = case_when(
    grepl("Trip_Blank_1", sample) ~ "KWF Crew, RM1.5_Kenai_City_Dock",
    grepl("Trip_Blank_2", sample) ~ "USFWS Crew, RM6.5_Cunningham_Park",
    grepl("Trip_Blank_3", sample) ~ "DEC Crew, RM40_Bings_Landing",
    grepl("Trip_Blank_4", sample) ~ "DEC Crew, RM43_Upstream_of_Dow_Island"))
  
  
# seperate result qualifiers (U, J, B) in to a new column
#sgs21 %<>%
  # account for fact that als data already has qualifier column and has characters in results column
 # mutate(qualifier = case_when(
  #  result == "ND" ~ qualifier,
  #  result != "ND" ~ str_extract(result,"[aA-zZ]+"))) %>%
#  mutate(result = str_remove(result,"[aA-zZ]+")) 




############## Part D: Prepare SGS/ALS Location/Site Names ##########################

# NOTE: The SGS and ALS 2021 results have a variety of misspelling and typos. For 2022, we should provide labs with a csv file of site names that they can use

# In preparation for a join to AQWMS table, we will manually generate a match table csv file that we can use 

## generate list of unique site names from 2021 SGS data
sgs21_sitenames <- data.table(unique(dat$sample)) %>%
  arrange(V1)
  

# generate list of unique site names from 2021 AQWMS template. these are the names we want in the final product
aqwms21_sitenames <- read_excel("other/input/AQWMS/AWQMS_KWF_Baseline_2021.xlsx", sheet = "Monitoring Locations") %>%
  select("Monitoring Location Name", "Monitoring Location ID") %>%
  distinct()

# write 2021 sgs site names to an excel file
site_match_table_path <- "other/input/AQWMS/sgs_site_names_matching_table.xlsx"
write.xlsx(sgs21_sitenames, site_match_table_path) 

# create an excel file with two sheets: a.) SGS site names, and b.) AQWMS site names
wb <- loadWorkbook(site_match_table_path)
addWorksheet(wb,"Sheet2")
writeData(wb,"Sheet2",aqwms21_sitenames)
saveWorkbook(wb,site_match_table_path,overwrite = TRUE)


# Using these two tables, we will manually create a new file titled "sgs_site_names_matching_table_manual_edit.xlsx" and manually match up the two disparate naming systems. 

# Performed manually by B Meyer March 18, 2022.

# append "Monitoring Location Name" and "Monitoring Location ID" info from WQX to spring 2021 SGS data

## read in site names join table
sitenames21_match <- read_excel("other/input/AQWMS/sgs_site_names_matching_table_manual_edit.xlsx") %>%
  select(`Monitoring Location Name`,`Monitoring Location ID`,sgs_sitenames) %>%
  rename(sample = sgs_sitenames) %>%
  filter(!is.na(`Monitoring Location ID`))

# append monitoring location names
dat %<>%
  left_join(sitenames21_match, by = "sample") %>%
  clean_names()

# remove dfs
rm(sgs21_sitenames,aqwms21_sitenames,sitenames21_match)



######################## Part E: "Result Analytical Method Context" name rectification ######################

# In the AQWMS template, the EPA names that will go in the column "Result Analytical Method ID" do not exactly match the names provided by the laboratory (SGS). After communicating with SGS and DEC on 2/8/2022, we are able to cross-walk between the two naming systems. These matches are documented in the excel file "analysis_code_matching_table.xlsx."

# assign "Result Analytical Method ID" and "Result Analytical Method Context" to dataset using matching table

# read in matching table
analysis_code_matching_table <- read_excel("other/input/AQWMS/analysis_code_matching_table.xlsx") %>%
  select(-Comments,-`EPA Name`) %>%
  clean_names() %>%
  rename(analytical_method = sgs_analysis_code) %>%
  # remove "EP" prefix from method "EP200.8"
  mutate(analytical_method = str_replace(analytical_method,"EP200.8","200.8"))
  

# read in AQWMS Analytical Methods list
aqwms_analytical_methods <- read_excel("other/input/AQWMS/AWQMS_KWF_Baseline_2021.xlsx", sheet = "Analytical Methods") %>%
  select("ID","Context Code") %>%
  clean_names() %>%
  rename(epa_analysis_id = id) %>%
  distinct()

# join two tables above
epa_analysis_codes <- inner_join(aqwms_analytical_methods,analysis_code_matching_table, by = "epa_analysis_id") %>%
  filter(!context_code %in% c("USEPA Rev 5.4",
                              "APHA (1997)",
                              "APHA (1999)")) 

# join EPA analysis IDs and context codes to overall dataset
dat %<>%
  mutate(analytical_method = str_replace(analytical_method,"EP200.8","200.8")) %>%
  left_join(epa_analysis_codes, by = "analytical_method") 


# remove unneeded dfs
rm(analysis_code_matching_table,aqwms_analytical_methods,epa_analysis_codes)



########################## Miscellaneous Steps #########################################


# move "DUP1" in sample_type column over to sample_condition as "DUP"
#dat %<>%
#  mutate(sample_condition = case_when(
#    sample_type == "DUP1" ~ "DUP",
#    sample_type != "DUP1" ~ sample_condition))


# segregate laboratory QA/QC data from field data. These data will be evaluated at a later step. See excel file "other/input/AQWMS/sample_type_abbreviations_manual_edit.xlsx" for sample_type naming schema
sgs21_als21_qaqc_dat <- dat %>%
    # retain only results not from field sampling program (project samples and trip blanks)
    # also filter out hydrocarbon surrogate results ("surr"). Surrogate standards are compounds spiked into all samples, blanks, Laboratory Control Samples, and matrix spikes to monitor the efficacy of sample extraction, chromatographic, and calibration systems. They do not represent environmental observations.
   filter(!sample_type %in% c("PS","SMPL","TB") | 
            grepl("(surr)",analyte))
write.csv(sgs21_als21_qaqc_dat, "other/output/lab_qaqc_data/2021_lab_qaqc_data/sgs21_als21_qaqc_dat.csv")


# retain only non-field sample results for AQWMS export
dat %<>%
  filter(sample_type %in% c("PS","SMPL","TB")) %>%
  filter(!grepl("(surr)",analyte))

```

</details>

<br>

------------------------------------------------------------------------

##### Fecal Coliform Lab Results (Soldotna Wastewater Treatment Plant (SWWTP)/Taurianen Engineering)

```{r, echo = F}
xfun::embed_file('other/input/2021_wqx_data/spring_2021_wqx_data/SWWTP/KRWF Fecal 05-11-21.xls', text = "Download Original Spring 2021 Fecal Coliform Lab Results from SWWTP")

```

```{r, echo = F}
xfun::embed_file('other/input/2021_wqx_data/summer_2021_wqx_data/Taurianen/FecalColiform_Results_Summer2021.pdf', text = "Download Original Summer 2021 Fecal Coliform Lab Results from Taurianen")

```

<details>

<summary>

*Show/Hide Code used to Prepare 2021 Fecal Coliform Results*

</summary>

```{r}

############################################################################################################
##################################### Read in  and Clean SWWTP / Taurianen FC Data #########################
############################################################################################################

########################### Part A: SWWTP FC Data Read In ##################################################
swwtp_spring21 <- read_excel("other/input/2021_wqx_data/spring_2021_wqx_data/SWWTP/KRWF Fecal 05-11-21.xls", skip = 11) %>%
  clean_names() %>%

## fix site naming and terminology

# move info about duplicate sample and/or sample blank status into separate column
  # sample type abbreviations
  mutate(sample_type = case_when(
    grepl("BLANK",sample_location_rm) ~ "MB", # method blank
    grepl("POSITIVE",sample_location_rm) ~ "LCS")) %>% # laboratory control sample
 # assign all other samples as "PS" (project sample)
  mutate_at(vars(sample_type),~replace_na(.,"PS")) %>%

  # field dup designation
  mutate(sample_condition = case_when(
    grepl("DUP",sample_location_rm) ~ "Field Duplicate")) %>%
  # remove "BLANK", and "POSITIVE designation from sample_location column
  mutate(sample_location_rm = (str_replace(sample_location_rm, "BLANK|POSITIVE", ""))) 


# remove "DUP" from site name column and trim white spaces in site name column
swwtp_spring21 %<>%
  mutate(sample_location_rm = str_remove(sample_location_rm,"DUP")) %>%
  mutate(sample_location_rm = str_trim(sample_location_rm,"right"))


# address different site naming systems
# use manually generated matching table
# read in matching table and match
swwtp_spring21_site_matching <- read_excel("other/input/AQWMS/swwtp_site_names_matching_table_manual_edit.xlsx") 

# join
swwtp_spring21 %<>%  
  full_join(swwtp_spring21_site_matching) %>%
  select(-sample_location_rm)

rm(swwtp_spring21_site_matching)


## fix lab analysis times and dates
swwtp_spring21 %<>%
  # lab processing time/date
  mutate(analysis_time_in = as_hms(time_in),
         analysis_date_in = mdy(spring21_sample_date),
         analysis_time_out = as_hms(time_out),
         # see file "other/input/2021_wqx_data/spring_2021_wqx_data/SWWTP/KRWF Fecal 05-11-21.xls for out analysis date
         analysis_date_out = ymd("2021-05-12")) %>%
  select(-time_in,-time_out) %>%
  transform(time_sampled = as_hms(time_sampled)) %>%
  # field sample date and time
  mutate(time_sampled = as_hms(time_sampled), 
         sample_date = mdy(spring21_sample_date))
  


## rename existing column names and create new ones to match sgs21 data format at end of prior code chunk
swwtp_spring21 %<>% 
  rename(lab_sample = dish_number,
         result = colony_count_100m_l,
         collect_time = time_sampled,
         run_time = analysis_time_in,
         run_date = analysis_date_in,
         # = analysis_time_out,
         # = analysis_date_out,
         collect_date = sample_date) %>%
  mutate(note = paste0("Lab analysis volume = ",ml," mL"),
         matrix = "Water (Surface, Eff., Ground)",
         analytical_method = "9222 D ~ Membrane filtration test for fecal coliforms",
         analyte = "Fecal Coliform",
         units = "cfu/100ml",
         # reporting limit ("loq") value from 2019 QAPP, pg 17
         loq = 1.0,
         lab_name = "Soldotna Wastewater Treatment Plant, Soldotna, Alaska",
         units = "cfu/100ml",
         epa_analysis_id = "9222D",
         context_code = "APHA",
         analyst = "AW") %>%
  clean_names() %>%
  select(-ml,-colony_count) %>%
  
  # transform to prep for bind with sgs21
  transform(lab_sample = as.character(lab_sample),
            result = as.double(result)) %>%
  
  # apply correction to the one "TNTC" result (Too Numerous To Count), since we can't have characters and integers in same column
  mutate(note = case_when(
    lab_sample == "30" ~ paste("Lab analysis volume = 0.5 mL, result = TNTC"),
    TRUE ~ note))



# segregate lab results from field results, and write lab qa/qc results to external csv
swwtp_spring21_qaqc_dat <- swwtp_spring21 %>%
  filter(sample_type %in% c("MB","LCS"))
write.csv(swwtp_spring21_qaqc_dat, "other/output/lab_qaqc_data/2021_lab_qaqc_data/swwtp_spring21_fc_qaqc_dat.csv", row.names = F)

swwtp_spring21 %<>%
  filter(!sample_type %in% c("MB","LCS"))


## join SGS 2021 data to Spring 2021 Fecal Coliform data from SWWTP

dat <- bind_rows(dat,swwtp_spring21) %>%
  select(-location)

rm(swwtp_spring21)



########################### Part B: Taurianen FC Data Read In (Summer 2021) ##############################################


## read in taurianen summer 2021 results
taur_summer21 <- read_excel("other/input/2021_wqx_data/summer_2021_wqx_data/Taurianen/Fecal_Coliform_Results_Spreadsheet.xlsx", skip = 3) %>%
  clean_names() %>%
  select(-qc1,-data_entry,-qc2) %>%
  
## move info about duplicate sample and/or sample blank status into separate column
  mutate(sample_condition = case_when(
    grepl("DUP",sample_location) ~ "Field Duplicate")) %>%
  # remove "DUP" designation from sample_location column
  mutate(sample_location = (str_replace(sample_location, "_DUP", ""))) %>%
  # trim white spaces in site name column
  mutate(sample_location = str_trim(sample_location,"right")) %>%
  
## add known info about times/dates, correct formats and column names
  mutate(collect_date = mdy(summer21_sample_date),
         run_date = mdy(summer21_sample_date),
         run_time = as_hms(time_relinquished),
         analysis_date_out = mdy("7/28/2021"),
         analysis_time_out = as_hms(time_tested), 
         # drop old columns
         .keep = "unused") %>%
  select(-date_of_testing,-neg_pos) %>%
  transform(time_sampled = as_hms(time_sampled)) %>%
  
## add lab name
  mutate(lab_name = "Taurianen Engineering and Testing, Soldotna, Alaska") %>%
  
  # rename columns
  rename(sample = sample_location,
         collect_time = time_sampled)

# NOTE: for Taurianan QA/QC practices, see email from from Taurianen at "other/documents/references/Taurianen QA Technique (Email march 2022).docx" (folder in this project repo)
  

## fix site naming and terminology

# generate spreadsheet of unique site names from taurianen dataset 
taur_summer21_sites <- data.frame(unique(taur_summer21$sample)) %>%
  rename(sample = unique.taur_summer21.sample.)

# export site names list to spreadsheet
write.xlsx(taur_summer21_sites, "other/input/AQWMS/taurianen_site_names_matching_table.xlsx")

# manually edit a new spreadsheet such that taurianen site names are paired iwth AWQMS site names
# read in manually edited site names sheet
taur_summer21_sites <- read_excel("other/input/AQWMS/taurianen_site_names_matching_table_manual_edit.xlsx") 

# join AWQMS site names to taurianen data
taur_summer21 <- left_join(taur_summer21,taur_summer21_sites,by = "sample") 



## add and/or rename other columns to match SWWTP dataframe structure
taur_summer21 %<>%
  clean_names() %>%
  select(-direct_count) %>%
  rename(result = number_of_colonies) %>%
  mutate(note = "",
         matrix = "Water",
         analytical_method = "9222 D ~ Membrane filtration test for fecal coliforms",
         analyte = "Fecal Coliform", 
         units = "cfu/100ml",
         # loq = reporting limit
         loq = 1,
         epa_analysis_id = "9222D",
         context_code = "APHA") %>%
  transform(result = as.double(result))


# assign "sample_type"
taur_summer21 %<>%  mutate(sample_type = case_when(
    sample_condition == "Lab Blank" ~ "MB", # method blank
    sample_condition == "Positive Control" ~ "LCS", # laboratory control sample
    TRUE ~ "PS"
    ))



# segregate lab results from field results, and write lab qa/qc results to external csv
taur_summer21_qaqc_dat <- taur_summer21 %>%
  filter(sample_type %in% c("MB","LCS"))
write.csv(taur_summer21_qaqc_dat, "other/output/lab_qaqc_data/2021_lab_qaqc_data/taur_summer21_qaqc_dat.csv", row.names = F)


# join 2021 Taurianen Fecal Coliform data into overall dataframe so far
dat <- bind_rows(dat,taur_summer21)
  

rm(taur_summer21,taur_summer21_sites,taur_summer21_qaqc_dat)

```

</details>

<br>

------------------------------------------------------------------------

##### Total Dissolved Solids Lab Results (Soldotna Wastewater Treatment Plant (SWWTP))

```{r, echo = F}
xfun::embed_file('other/input/2021_wqx_data/spring_2021_wqx_data/SWWTP/KRWF TSS MONITORING 05-11-21.xlsx', text = "Download Original Spring 2021 Total Suspended Solids Results from SWWTP.xlsx")
```

```{r, echo = F}
xfun::embed_file('other/input/2021_wqx_data/summer_2021_wqx_data/SWWTP/KRWF TSS MONITORING 07-28-21.xlsx', text = "Download Original Summer 2021 Total Suspended Solids Results from SWWTP.xlsx")
```

<details>

<summary>

*Show/Hide Code used to Prepare 2021 Total Dissolved Solids Results*

</summary>

```{r}
# SWWTP Spring 2021 TSS data
## Reformat TSS data to match AQWMS template


# read in
swwtp_tss_spring21 <- read_excel('other/input/2021_wqx_data/spring_2021_wqx_data/SWWTP/KRWF TSS MONITORING 05-11-21.xlsx', skip = 1, sheet = "Updated_Formatting") %>%
  clean_names() %>%
  transform(date_of_analysis = anydate(date_of_analysis)) %>%
  # add info from lab COC
  mutate(rec_date = ymd_hms("2021-05-11 14:00:00"))

swwtp_tss_summer21 <- read_excel('other/input/2021_wqx_data/summer_2021_wqx_data/SWWTP/KRWF TSS MONITORING 07-28-21.xlsx', skip = 1, sheet = "Updated_Formatting") %>%
  clean_names() %>%
  transform(sample_time = anytime(sample_time)) %>%
  # add info from lab COC
  mutate(rec_date = ymd_hms("2021-07-27 14:00:00"))


# combine spring & summer
swwtp_tss21 <- bind_rows(swwtp_tss_spring21,swwtp_tss_summer21) %>%
  remove_empty() 
rm(swwtp_tss_spring21,swwtp_tss_summer21)

# prepare and format to match larger dataset

 # miscellaneous

swwtp_tss21 %<>%
  select(-qc1,-data_entry,-x8) %>%
  rename(analysis_time = time) %>%
  transform(sample_time = as_hms(sample_time),
            analysis_time = as_hms(analysis_time)) %>%

  # move info about duplicate sample and/or sample blank status into separate column
  mutate(sample_condition = case_when(
    grepl("DUP",sample_location) ~ "Field Duplicate")) %>%
  # remove "DUP" designation from locations column
  mutate(sample_location = str_replace(sample_location, "_DUP", "")) %>%
  # replace "O" with zeros in location column
  mutate(sample_location = str_replace(sample_location, "RM_O", "RM_0")) %>%
  # add units of suspended solids
  mutate(units = "mg/l") %>%
  rename(result = s_s_mg_l) %>%
  transform(result = as.numeric(result)) %>%
  
  # add info about EPA analysis type from AWQMS template
  mutate(epa_analysis_id = "2540-D",
         analytical_method = "SM21-2540-+D",
         context_code = "APHA",
         note = "") %>%
  
  # remove tare and paper weight values
  select(-dried_wt,-paper_wt,-tare_wt_kg, -ml) %>%
  
  # modify date/time formats
  mutate(collect_date = as.character(paste(field_sample_date,sample_time)),
         run_date_time = as.character(paste(date_of_analysis,analysis_time)), .keep = "unused") %>%
  mutate(collect_time = as_hms(as.POSIXct(collect_date))) %>%
  mutate(collect_date = date(as.POSIXct(collect_date)),
         run_time = as_hms(ymd_hms(run_date_time)),
         run_date = date(ymd_hms(run_date_time)),.keep = "unused") %>%
  
  # rename
  rename(analyst = signature) %>%
  
  # miscellaneous
  mutate(lab_sample = "",
         matrix = "Water",
         analyte = "Total suspended solids",
         # loq = reporting limit
         loq = 1.0,
         lab_name = "Soldotna Wastewater Treatment Plant, Soldotna, Alaska")

  
# assign "sample_type"
swwtp_tss21 %<>%  
  mutate(sample_type = case_when(
    sample_condition == "Lab Blank" ~ "MB", # method blank
    sample_condition == "Positive Control" ~ "LCS", # laboratory control sample
    TRUE ~ "PS"
    ))


  
# get site names consistent with AWQMS format
swwtp_tss_sitenames <- data.frame(unique(swwtp_tss21$sample_location))

# delete existing csv if present
unlink("other/input/AQWMS/swwtp_tss_sitenames.csv")

# export csv of swwtp_tss site names
write.csv(swwtp_tss_sitenames,"other/input/AQWMS/swwtp_tss_sitenames.csv",row.names = F)

# use this list to create manually edited file, matched to AWQMS template names
# read in manually edited file
swwtp_tss_sitenames <- read_excel("other/input/AQWMS/swwtp_tss_site_names_matching_table_manual_edit.xlsx")

# join correct site names to overall 2021 TSS dataset
swwtp_tss21 <- left_join(swwtp_tss21,swwtp_tss_sitenames)  %>%
  clean_names() %>%
  rename(sample = sample_location)


######## 3/21/2022 --> no qa/qc results from SWWTP TSS on record yet. contact by email to see whats available. looking for lab duplicate and/or external qc check standard (pg 34 of QAPP)

# no response as of 3/29/22. need to call to ensure practice/records are in place by spring 2022

# segregate lab results from field results, and write lab qa/qc results to external csv
#swwtp_tss21_qaqc_dat <- swwtp_tss21 %>%
#  filter(sample_type %in% c("MB","LCS"))
#write.csv(swwtp_tss21_qaqc_dat, "other/output/lab_qaqc_data/2021_lab_qaqc_data/swwtp_tss21_qaqc_dat.csv", row.names = F)


#rm(swwtp_tss21_qaqc_dat)


# join TSS data with overall dataset
dat <- bind_rows(dat,swwtp_tss21)

rm(swwtp_spring21,swwtp_tss_sitenames,swwtp_tss21)

```

</details>

------------------------------------------------------------------------

<details>

<summary>

*Additional Miscellaneous Code Used to Prepare Data For AQWMS Template*

</summary>

```{r}

############### Miscellaneous Steps for Overall Field Results Dataframe #################


############# a.)  filter out lab blanks and positive lab controls ##################
#dat %<>%
#  filter(!sample_condition %in% c("Lab Blank","Positive Control")) # should already be gone now 3/4/22


############ b.) match latitude and longitude coordinates to sites ##################

## read in coordinates
site_coords <- read_excel("other/input/AQWMS/AQWMS_template_matching_table.xlsx", sheet = "site_coordinates") %>%
  remove_empty()

## join coords to overall df
dat <- left_join(dat,site_coords)


############ c.) assign "result sample fraction" (e.g. filtered, dissolved, etc.) ############

## read in manually organized table that pairs "result sample fraction" with "analytical method"
result_sample_fraction <- read_excel("other/input/AQWMS/AQWMS_template_matching_table.xlsx", sheet = "result_sample_fraction") %>%
  filter(!is.na(analytical_method)) %>%
  select(-description)

## join to table
dat <- left_join(dat,result_sample_fraction)



################ d.) assign "result detection condition"##############

## read in manually assigned join table
result_detection_condition <- read_excel("other/input/AQWMS/AQWMS_template_matching_table.xlsx", sheet = "result_detection_condition") %>%
  clean_names() %>%
  filter(!is.na(resultflag))

## join to table
## z <- dat %>% left_join(dat,result_detection_condition, by = "resultflag")

## the attempt at left_join above is exhibiting nonsensical result for unclear reason. For now, define programmatically:

dat %<>%
  mutate(result_detection_condition = case_when(
    resultflag == "U" ~ "Not Detected",
    resultflag == "J" ~ "Present Below Quantification Limit"))


############### e.) assign chemical preservative type ################

## read in table
chemical_preservative <- read_excel("other/input/AQWMS/AQWMS_template_matching_table.xlsx", sheet = "chemical_preservative") %>%
  filter(!is.na(preservative)) %>%
  select(-description)

## join to overall dataframe
dat <- left_join(dat,chemical_preservative)


############# f.) assign bottle type and color ######################

## read in table
bottle_type_color <- read_excel("other/input/AQWMS/AQWMS_template_matching_table.xlsx", sheet = "sample_container_type_color") %>%
  select(-description) %>%
  filter(!is.na(sample_container_type))

## join to overall dataframe
dat <- left_join(dat,bottle_type_color)

rm(site_coords, result_sample_fraction, result_detection_condition, chemical_preservative, bottle_type_color)



############ g.) assign "Statistical Base Code" column ##############

# this value is not applicable to most results here, usually left blank. will assign "count": to fecal coliform

dat %<>%
  mutate(stat_base_code = case_when(
    analyte == "Fecal Coliform" ~ "Count"
  ))




########### f.) "Activity ID" code shortening ######################

# The Activity ID column consists of (`Monitoring Location ID`,"-",collect_date,"-",analyte,"-",sample_condition). This field is permitted to be no longer than 55 characters. With full names of analytes included, the entries sometimes exceed 55 characters. To address this, we will use analyte abbreviations. For single elements, we will use their periodic table abbreviation. For other analytes (e.g. nutrients, volatiles), we will consult what abbreviations already exist in the Water Quality Exchange repository

# export list of unique analytes from 2021 data
write.csv(data.frame(unique(dat$analyte)),"other/input/AQWMS/analytes_list.csv", row.names = F)

# manually...

# re-import edited list of analyte abbreviation names


```



```{r}
############ XX.) prepare final format ###############################

# create column structure from example in AQWMS template. Use existing input from SGS results if applicable, specify value from "Permitted Values" tab if SGS input not applicable or not yet specified

dat %<>%
  # Proceeding left to right across columns of AWQMS template
  # Mutate new column or rename existing column as needed
  
  mutate(
    `Monitoring Location ID` = monitoring_location_id,
    `Activity Media Name` = "Water",
    `Activity Media Subdivision Name` = "Surface Water",
    # create activity ID name conditionally if condition(s) present
    `Activity ID` = case_when(
      is.na(sample_condition) ~ paste0(`Monitoring Location ID`,"-",collect_date,"-",analyte),
      !is.na(sample_condition) ~ paste0(`Monitoring Location ID`,"-",collect_date,"-",analyte,"-",sample_condition)),
    `Activity Start Date` = collect_date,
    `Activity Start Time` = collect_time,
    `Activity End Date` = "",
    `Activity End Time` = "",
    `Activity Latitude` = latitude, 
    `Activity Longitude` = longitude, 
    `Activity Source Map Scale` = "",
    `Activity Type` = case_when(
      sample_condition == "Field Duplicate" ~ "Quality Control Field Replicate Msr/Obs",
      sample_condition == "Blank" ~ "Quality Control Sample-Trip Blank",
      sample_type == "TB" ~ "Quality Control Sample-Field Blank",
      TRUE ~ "Field Msr/Obs"),
    # All samples are surface grab samples. Depths are assigned across the board here as 6 inches (~15 cm) 
    `Activity Depth/Height Measure` = 15,
    `Activity Depth/Height Unit` = "cm",
    # Next three columns not applicable for surface grab samples
    `Activity Top Depth/Height Measure` = "",
    `Activity Top Depth/Height Unit` = "",
    `Activity Bottom Depth/Height Measure` = "",
    `Activity Bottom Depth/Height Unit` = "",
    `Activity Relative Depth Name` = "",
    `Activity Comment` = note,
    `Characteristic Name` = analyte,
    `Result Analytical Method ID` = epa_analysis_id,
    `Result Analytical Method Context` = context_code,
    `Method Speciation` = "",
    `Result Value` =  result,
    `Result Unit` = units,
    `Result Qualifier` = resultflag,
    `Result Weight Basis` = "Sampled",
    `Statistical Base Code` = stat_base_code,
    `Result Sample Fraction` = result_sample_fraction, 
    `Result Value Type` = "Actual",
    `Result Comment` = "",
    `Sample Collection Method ID` = "",
    `Equipment ID` = "Water Bottle",
    `Result Detection Condition` = result_detection_condition,
   
   # potentially a complex process here 3/2/2022 - sent email to DEC to clarify how to deal w/ duplicate column names
   # reply: renaming as 1 and 2 OK for now
   
    `Result Detection Limit Type 1` = "Limit of Quantitation",
    `Result Detection Limit Value 1` = loq,
    `Result Detection Limit Unit 1` = units,
   # note: lod = "limit of detecion"; equivalent to "method detection level"
    `Result Detection Limit Type 2` = "Method Detection Level",
    `Result Detection Limit Value 2` = lod,
    `Result Detection Limit Unit 2` = units,
    `Laboratory Accreditation Indicator` = "",
    `Laboratory Name` = lab_name,
    `Laboratory Sample ID` = lab_sample,
    `Analysis Start Date` = run_date,
    `Analysis Start Time` = run_time,
    `Biological Intent` = "",
    `Subject Taxonomic Name` = "",
    `Thermal Preservative` = "Cold packs",
    `Sample Container Type` = sample_container_type,
    `Sample Container Color` = sample_container_color,
    `Chemical Preservative` = preservative
    # remove columns that were mutated to a new name
    ,.keep = "unused")

   
# next, we want top retain just those columns listed in the AQWMS template
# found solution to this problem here: https://gist.github.com/djhocking/62c76e63543ba9e94ebe

# get all column names from AQWMS template
aqwms_colnames <- read_excel("other/input/AQWMS/AWQMS_KWF_Baseline_2021.xlsx", sheet = "KWF Baseline AWQMS Template") %>%
  colnames()

# select from subset of column in aqwms template
 dat %<>%
  select(one_of(aqwms_colnames))
 
# export final formatted AQWMS results to external csv
write.csv(dat,"other/output/aqwms_formatted_results/2021_kwf_baseline_results_aqwms.csv",row.names = F)






#### to do

# figure out whats the deal with BTEX surrogate ("surr") results

```

</details>

<br>

------------------------------------------------------------------------

<br>

### Provisional Results, Prior to Data Review

*Results last updated `r Sys.Date()`*

```{r, echo = F}
xfun::embed_file('other/output/aqwms_formatted_results/2021_kwf_baseline_results_aqwms.csv', text = "Download All Provisional 2021 Kenai River Baseline Water Quality Monitoring Results, Formatted for AQWMS")

```

<br>

### Data Evaluation

Prior to submission to ADEC, all water quality data must address a series of data evaluation questions in order to evaluate how quality assurance / quality check (QA/QC) processes are instituted. The draft Data Evaluation Checklist Template (available below) outlines these questions:

```{r, echo = F}
xfun::embed_file('other/documents/AQWMS_documents/Data_Evaluation_Checklist_Template_20220201.docx', text = "Download Draft ADEC Draft Data Evaluation Checklist Template")
```

#### Pre-Database

##### Overall Project Success

```{r, echo = F}

# This code chunk will prepare values to be used in addressing the data review questions below. 

# The code will be executed here rather than in individual chunks above each individual question, because some of the questions are not sequentially independent. 

# That is to say, values from multiple questions are needed to answer single questions, sometimes from prior and sometimes from after a particular question. For example in question #4, we want to know how many values we excluded from the whole original dataset, which requires decisions based on questions following question #4.

# 1.: No calculation needed.
# 2.: No calculation needed.
# 3.: No calculation needed.
# 4.: Calculations needed

## 4.1: Completeness Measure A: create count table of collected results in original aqwms file. flag those results that are not submitted to ADEC. compare two sets of values to get completeness measure A.

### table of samples collected
total_samples_collected_2021 <- read.csv('other/output/aqwms_formatted_results/2021_kwf_baseline_results_aqwms.csv') %>%
  clean_names() %>%
  # unique location, date, parameter
  distinct(monitoring_location_id, activity_start_date, characteristic_name, result_value) %>%
  count() %>%
  as.integer()


### table of samples finalized and submitted to DEC



## 4.2: Completeness Measure B: use table for kit request to get total number of planned samples. compare this table to final set of values that hold up against qa/qc process
```






1.  Did the project follow the QAPP? ***Yes***. Were there any deviations from the sampling plan? ***Yes. Refer to Individual Answers Below***

2.  Was the data collected representative of environmental conditions? ***Yes***. Notes: Study design is intended to create two single-day snapshots across a wide geographical area of the Kenai River watershed.

3.  Are site names, dates, and times correct and as expected? ***Yes, after post-season correction*** Notes: In 2021 Kenai Watershed Forum used pre-printed labels on all sample bottles, reducing opportunity for field and lab transcription errors. Remaining site name transcription errors were corrected in post-season data review.

```{r, echo = F}

# we have identified errors in site name / sample type transcription
# to address this, we will manually create a transmission chain of site/sample names for spring 2021
# see excel document at other/input/2021_wqx_data/miscellaenous/2021_sample_name_rectification.xlsx

# task: go though and document chain of custody to decide what data we can defend inclusion/exclusion of

## 2021 SGS site/sample names
# read in 2021 sgs data
spring_batch_sgs21 <- read.csv("other/input/2021_wqx_data/spring_2021_wqx_data/SGS/spring_2021_sgs_batch_info.csv")
summer_batch_sgs21 <- read.csv("other/input/2021_wqx_data/summer_2021_wqx_data/SGS/summer_2021_sgs_batch_info.csv")

# clean up and retain only useful columns
sgs21 <- bind_rows(spring_batch_sgs21,summer_batch_sgs21) %>%
  clean_names() %>%
  filter(project_id == "KenaiRiver Wtr Qlty Monitoring") %>%
  distinct(sample_id)
write.csv(sgs21,"other/temp.csv")

## 2021 ALS site/sample names
#### read in spring 2021 results from ALS 
spring_als21 <- read.csv("other/input/2021_wqx_data/spring_2021_wqx_data/SGS/spring_2021_als_batch_info.csv") %>%
    clean_names() 
summer_als21 <- read.csv("other/input/2021_wqx_data/summer_2021_wqx_data/SGS/summer_2021_als_batch_info.csv") %>%
    clean_names()

# bind spring and summer 2021 ALS
als21 <- bind_rows(spring_als21,summer_als21) %>%
  distinct(sample)



```




4.  Is the dataset complete and did you receive the expected number of results? ***TBD***. Notes: we acquired the expected number of results in 2021 sampling events. All sites described in the QAPP were visited twice in 2021, and all water sample collections were successfully conducted. However most intrinsic water quality parameters mentioned in the 2020 QAPP were not collected in 2021.

Data evaluation is currently in progress and the values below are current as of `r Sys.Date()`.


-   Completeness Measure A: (Samples Collected) / (Samples Submitted to ADEC) \*100% = ***%TBD***

    -   Project goal: ***85%***

-   Completeness Measure B: (Planned Number of Samples) / (Useable Samples) \* 100% = ***%TBD***

    -   Project goal: ***60%***


5.  Were field duplicates, blanks, and/or other QC samples collected as planned? ***No***

```{r, echo = F}
# How many field blanks, field duplicates, and other QC samples were planned vs. collected?

################ A.) Count PLANNED 2021 field blanks/dups #################

# use kit request data to count "required" (planned) samples

## read in SPRING 2021 planned samples
spring21_planned <- read_excel("other/input/2021_wqx_data/spring_2021_wqx_data/SGS/KRBWQM_SGS_bottle_order_Spring_2021.xlsx", sheet = "bottles_analyses_spring_2021") %>%
  mutate(activity_start_date = spring21_sample_date)

## read in SUMMER 2021 planned samples
summer21_planned <- read_excel("other/input/2021_wqx_data/summer_2021_wqx_data/SGS/KRBWQM_SGS_bottle_order_Summer_2021.xlsx", sheet = "bottles_analyses_summer_2021") %>%
  mutate(activity_start_date = summer21_sample_date)

## combine spring and summer 2021
planned_samples_2021 <- bind_rows(spring21_planned,summer21_planned) %>%
  
## prepare and summarise table samples collected and tests performed
  clean_names() %>%
  filter(item == "Bottle") %>%
  select(site,container_size,quantity,preservative,analysis_1,analysis_2,activity_start_date) %>%
  fill(site, .direction = "down") %>%
  filter(site != "Spare Kit") %>%
  mutate(sample_type = case_when(
    grepl("DUPLICATE",site, ignore.case = TRUE) ~ "FIELD DUPLICATE",
    grepl("blank",analysis_1, ignore.case = TRUE) ~ "FIELD BLANK"
  )) 

### make available for download? ... "planned sample collection 2021"


## count planned field replicates
planned_field_dups_2021 <- planned_samples_2021 %>%
  filter(sample_type == "FIELD DUPLICATE") %>%
  count() %>%
  as.character()

## count planned field blanks 
planned_field_blanks_2021 <- planned_samples_2021 %>%
  filter(sample_type == "FIELD BLANK") %>%
  count() %>%
  as.character()

# remove
rm(spring21_planned,summer21_planned)


################ B.) Count ACTUALLY COLLECTED 2021 field blanks/dups #################

z <- dat %>%
  clean_names() %>%
  filter(activity_type != "Field Msr/Obs") %>%
  distinct(activity_type,result_analytical_method_id, activity_start_date, monitoring_location_id)
  
  
# for unclear reasons,  DUP status is incorrectly assigned to the following:
# KR RM 22 SOC 2021-05-11 200.7
# KR RM 1.5 2021-07-27 200.7
# KR RM 23 2021-05-11 200.7
# KR RM 70 2021-07-27 200.7

# likely explanation -- als ran these as LAB duplicates ... was this even requested or part of the qapp????? yeesh.
# steps:
# 1.) ensure data is transferred from EDD to final export (currently is not, eg. average = "NA", rpd is blank)
# 2.) assign these data as LAB replicates. wtf.


  
# 3/28/22 - looked back in earlier code chunks to find why  

  
  # assign each observation as dup or blank
  mutate(sample_type = case_when(
    grepl("DUP",activity_id, ignore.case = TRUE) ~ "FIELD DUPLICATE",
    grepl("blank",activity_id, ignore.case = TRUE) ~ "FIELD BLANK")) %>%
  select(activity_type,sample_type,result_analytical_method_id, activity_start_date, monitoring_location_id) %>%
  distinct() %>%
  # rearrange dataframe so that both phosphorus and nitrate tests are from same bottle
  filter(result_analytical_method_id != "4500-P-E") %>%
  rename(result_analytical_method_id_1 = result_analytical_method_id) %>%
  mutate(result_analytical_method_id_2 = case_when(
    result_analytical_method_id_1 == "4500-NO3(F)" ~ "4500-P-E"
  )) %>%
  select(activity_type,sample_type,activity_start_date,monitoring_location_id,result_analytical_method_id_1,result_analytical_method_id_2) %>%
  arrange(monitoring_location_id,activity_start_date)

write.csv(z,"other/output/temp.csv")


# clarify issue here




# duplicate samples do not line up as expected.
# working here 3/23/2022; working to ID why three extra 200.7 samples appear. see output/temp.csv




# plan: do an anti_join with planned_samples_2021; find whats up, go back to arrange SGS code to repair...?!
 planned_qaqc_samples_2021 <- planned_samples_2021 %>%
   filter(!is.na(sample_type)) %>%
   select(site,analysis_1,analysis_2,activity_start_date)





## general plan: modify kit request to distinguish sample vs. above types. Create df of planned values (impr), join with actual sample summary value derived from aqwms pre-review submission
```

-   Field duplicates:

    -   \# required: ***`r planned_field_dups_2021`***
    -   \# collected

-   Field blanks:

    -   \# required: ***`r planned_field_blanks_2021`***
    -   \# collected

-   Other:

    -   \# required: NA
    -   \# collected: NA

6.  Are the duplicate sample(s) RPD within range described in QAPP? ***TBD***

-   RPD goal from QAPP(%): ***Varies by Parameter, See Current QAPP***
-   View duplicate RPD calculations and a summary table in supporting excel file; including parameters, site names, dates, results, and RPD values: ***Values TBD***

```{r, echo= F}
# generate table of rpd values and make csv downloadable
```

##### In-situ Field Data and Instruments

7.  Were there any issues with instrument calibration? ***No***. Did the instrument perform as expected? ***Yes***. Notes: ***TBD***

8.  Was instrument calibration performed according to the QAPP and instrument recommendations? ***Yes***. Were calibration logs or records kept? ***Yes***. Notes: ***Calibration records will be available for download here***.

9.  Was instrument verification during the field season performed according to the QAPP and instrument recommendations? ***TBD*** . Were verification logs or records kept? ***Yes***.

10. Do the instrument data files site IDs, time stamps and file names match? ***No electronic log files recorded***.

11. Is any field data rejected and why? ***TBD***.

##### Analytical Laboratory Reports and Results

12. Do the laboratory reports provide results for all sites and parameters? ***Yes***.

13. Were the appropriate analytical methods used for all parameters? ***Yes***.

14. Do the laboratory reports match the COC and requested methods? ***TBD***. Are same methods used throughout? ***TBD***.

15. Are the number of samples on the laboratory reports the same as on the COC? ***TBD***.

16. Is a copy of the COC included with the laboratory reports? ***Yes***. GET COC FOR SWWTP 2021...

17. Were preservation, hold time and temperature requirements met? ***Yes***. **Can provide calculation table to verify if necessary**.

18. Are there any project specific concerns (e.g. total v. dissolved, MST, etc)? ***TBD***.

19. Was all supporting info provided in the laboratory report, such as reporting limits for all analyses and definitions? ***Yes***.

20. Were there any laboratory discrepancies, errors, data qualifiers, or QC failures (review laboratory duplicates, matrix spikes and blanks)? ***TBD***.

21. Is any laboratory data rejected and why? ***TBD***.

22. Was the QA Officer consulted for any data concerns? ***TBD***.

<br>

*Subsequent steps in the "Database Prep" and "Database Import" phases are conducted by ADEC staff. See the above Data Evaluation Checklist Template for details.*

```{r, eval = F, echo = F}
# check out example from other WQP download
z1 <- read.csv("other/input/wqp_data/narrowresult.csv") %>%
  #filter(CharacteristicName == "Fecal Coliform")
 # select(ActivityStartDate,CharacteristicName) %>%
  clean_names() %>%
  select()
  transform(activity_start_date = ymd(activity_start_date)) %>%
  group_by(characteristic_name) %>%
  summarise(min_date = min(activity_start_date),
            max_date = max(activity_start_date))

### 1st step:::: make sure that monitoring list locations matches those currently match those in DEC AQWMS database (AWQMS public login: https://awqms2.goldsystems.com/Login.aspx, username akpublic, no password)

z2 <- read.csv("other/input/wqp_data/narrowresult.csv") %>%
  clean_names() %>%
  select(result_detection_condition_text,)
  select(characteristic_name, result_sample_fraction_text,result_analytical_method_method_name) %>%
  distinct()


```

```{r, notes, include = F}

# course of action

# uplift 2021 data

# audit existing data in WQX

# confirm with DEC that they are addressing data 2017 - 2020

# attempt to download missing data from  AQWMS, then upload to EPA WQX

# if not available, reshape data from shared drive to uplift to EPA WQX (big task)

```

<br>

```{r, other notes, include = F}

# ultimately, would like to set up auto query of EPA WQX so this report updates automatically (github actions or other methods...?); eg see front matter guts of https://geocompr.robinlovelace.net/.

```
